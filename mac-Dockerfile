# Base image from https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/deployment/resource-providers/standalone/docker/
FROM arm64v8/flink:1.19.1-scala_2.12-java17

# Build argument(s)
ARG FLINK_LANGUAGE
ARG AWS_ACCESS_KEY_ID
ARG AWS_SECRET_ACCESS_KEY
ARG AWS_REGION
ARG AWS_S3_BUCKET

# Map build argument(s) to container environment variable(s)
ENV FLINK_LANGUAGE=${FLINK_LANGUAGE}
ENV AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
ENV AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
ENV AWS_REGION=${AWS_REGION}
ENV AWS_S3_BUCKET=${AWS_S3_BUCKET}

# Create/set version number container environment variable(s)
ENV APACHE_FLINK_VERSION=1.19
ENV APACHE_FLINK_PATCH=.1
ENV APACHE_FLINK_VERSION_WITH_PATCH=${APACHE_FLINK_VERSION}${APACHE_FLINK_PATCH}
ENV PYTHON_VERSION=3.11
ENV PYTHON_PATCH=.9
ENV PYTHON_VERSION_WITH_PATCH=${PYTHON_VERSION}${PYTHON_PATCH}
ENV HADOOP_VERSION=3.4.0
ENV ICEBERG_VERSION=1.6.1

# Create/set folder contianer environment variable(s)
ENV FLINK_CONF_DIR=/opt/flink/conf
ENV FLINK_LIB_DIR=/opt/flink/lib/

# Create/set URL contianer environment variable(s)
ENV MAVEN_ROOT_URL=https://repo1.maven.org/maven2/org/apache/

# Container metadata
LABEL maintainer="j3@thej3.com" \
      description="Apache Flink ${APACHE_FLINK_VERSION_WITH_PATCH} container with Python ${PYTHON_VERSION_WITH_PATCH} installed."

# Install Python dependencies and whatever utilities are needed to install Python from source
# Install libbz2-dev, a development package for the bzip2 library (libbz2), which is used for compressing and decompressing data using the Burrows-Wheeler block-sorting text compression algorithm and Huffman coding.
# Install libffi-dev, a development package for the libffi library, which provides a portable, high-level programming interface to various calling conventions.
# Install libssl-dev, a development package for the OpenSSL library, which is a general-purpose cryptography library that provides an open-source implementation of the Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols.
# Install zlib1g-dev, a development package for the zlib library, which is a software library used for data compression.
# Install openjdk-17-jdk-headless, a headless version of the OpenJDK 17 Java Development Kit (JDK) that does not include any graphical user interface (GUI) libraries or tools.
RUN apt-get update && apt-get install -y \
    build-essential \
    openjdk-17-jdk-headless \
    zlib1g-dev \
    libbz2-dev \
    libpq-dev \
    libncurses5-dev \
    libgdbm-dev \
    liblzma-dev \
    libnss3-dev \
    libssl-dev \
    libsqlite3-dev \
    libreadline-dev \
    libffi-dev \
    wget \
    git \
    python3-pip \
    python3-venv \
    zip \
    nano

# Download and install the Flink libraries and plugins for use when writing Python-based Flink Apps
RUN if [ "$FLINK_LANGUAGE" = "python" ]; then \
    curl -L "https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-10.0/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar" -o "/opt/flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar" && \
    curl -L "${MAVEN_ROOT_URL}flink/flink-s3-fs-hadoop/${APACHE_FLINK_VERSION_WITH_PATCH}/flink-s3-fs-hadoop-${APACHE_FLINK_VERSION_WITH_PATCH}.jar" -o "${FLINK_LIB_DIR}flink-s3-fs-hadoop-${APACHE_FLINK_VERSION_WITH_PATCH}.jar" && \
    curl -L "${MAVEN_ROOT_URL}iceberg/iceberg-flink-runtime-${APACHE_FLINK_VERSION}/${ICEBERG_VERSION}/iceberg-flink-runtime-${APACHE_FLINK_VERSION}-${ICEBERG_VERSION}.jar" -o "${FLINK_LIB_DIR}iceberg-flink-runtime-${APACHE_FLINK_VERSION}-${ICEBERG_VERSION}.jar" && \
    curl -L "${MAVEN_ROOT_URL}iceberg/iceberg-aws-bundle/${ICEBERG_VERSION}/iceberg-aws-bundle-${ICEBERG_VERSION}.jar" -o "${FLINK_LIB_DIR}iceberg-aws-bundle-${ICEBERG_VERSION}.jar" && \
    curl -L "${MAVEN_ROOT_URL}hadoop/hadoop-common/${HADOOP_VERSION}/hadoop-common-${HADOOP_VERSION}.jar" -o "${FLINK_LIB_DIR}hadoop-common-${HADOOP_VERSION}.jar" && \
    curl -L "${MAVEN_ROOT_URL}flink/flink-sql-connector-kafka/3.2.0-${APACHE_FLINK_VERSION}/flink-sql-connector-kafka-3.2.0-${APACHE_FLINK_VERSION}.jar" -o "${FLINK_LIB_DIR}flink-sql-connector-kafka-3.2.0-${APACHE_FLINK_VERSION}.jar" && \
    curl -L "${MAVEN_ROOT_URL}flink/flink-connector-kafka/3.2.0-${APACHE_FLINK_VERSION}/flink-connector-kafka-3.2.0-${APACHE_FLINK_VERSION}.jar" -o "${FLINK_LIB_DIR}flink-connector-kafka-3.2.0-${APACHE_FLINK_VERSION}.jar" && \
    curl -L "${MAVEN_ROOT_URL}flink/flink-json/${APACHE_FLINK_VERSION_WITH_PATCH}/flink-json-${APACHE_FLINK_VERSION_WITH_PATCH}.jar" -o "${FLINK_LIB_DIR}flink-json-${APACHE_FLINK_VERSION_WITH_PATCH}.jar" && \
    curl -L "${MAVEN_ROOT_URL}kafka/kafka-clients/3.7.0/kafka-clients-3.7.0.jar" -o "${FLINK_LIB_DIR}kafka-clients-3.7.0.jar" && \
    cd /usr/local && \
    wget https://www.python.org/ftp/python/${PYTHON_VERSION_WITH_PATCH}/Python-${PYTHON_VERSION_WITH_PATCH}.tgz && \
    tar -xf Python-${PYTHON_VERSION_WITH_PATCH}.tgz && \
    cd Python-${PYTHON_VERSION_WITH_PATCH} && \
    ./configure --enable-optimizations && \
    make -j$(nproc) && \
    make altinstall && \
    ln -s /usr/local/Python-${PYTHON_VERSION_WITH_PATCH} /usr/bin/python && \
    python${PYTHON_VERSION} -m pip install --upgrade pip && \
    python${PYTHON_VERSION} -m pip install pipenv && \
    python${PYTHON_VERSION} -m venv .venv && \
    . .venv/bin/activate && \
    pipenv --python ${PYTHON_VERSION} install "grpcio-tools>=1.29.0,<=1.50.0" && \
    pipenv --python ${PYTHON_VERSION} install setuptools>=37.0.0 && \
    pipenv --python ${PYTHON_VERSION} install apache-flink==${APACHE_FLINK_VERSION_WITH_PATCH} && \
    pipenv --python ${PYTHON_VERSION} install pyiceberg && \
    pipenv --python ${PYTHON_VERSION} install boto3 && \
    pipenv --python ${PYTHON_VERSION} install s3fs && \
    pipenv --python ${PYTHON_VERSION} install google-api-python-client && \
    pipenv --python ${PYTHON_VERSION} install pyflink && \
    pipenv --python ${PYTHON_VERSION} install py4j==0.10.9.7 && \
    pip install boto3 && \
    pip install apache-flink==${APACHE_FLINK_VERSION_WITH_PATCH} && \
    pip install pyflink && \
    pip install google-api-python-client && \
    pip --no-cache-dir install --upgrade awscli && \
    export PATH="/usr/local/Python-${PYTHON_VERSION_WITH_PATCH}:${PATH}"; \
fi

# Download and install the Flink libraries and plugins for use when writing Java-based Flink Apps
RUN if [ "$FLINK_LANGUAGE" = "java" ]; then \
    curl -L "${MAVEN_ROOT_URL}iceberg/iceberg-flink-runtime-${APACHE_FLINK_VERSION}/${ICEBERG_VERSION}/iceberg-flink-runtime-${APACHE_FLINK_VERSION}-${ICEBERG_VERSION}.jar" -o "${FLINK_LIB_DIR}iceberg-flink-runtime-${APACHE_FLINK_VERSION}-${ICEBERG_VERSION}.jar" && \
    curl -L "${MAVEN_ROOT_URL}flink/flink-sql-connector-hive-3.1.3_2.12/${APACHE_FLINK_VERSION_WITH_PATCH}/flink-sql-connector-hive-3.1.3_2.12-${APACHE_FLINK_VERSION_WITH_PATCH}.jar" -o "${FLINK_LIB_DIR}flink-sql-connector-hive-3.1.3_2.12-${APACHE_FLINK_VERSION_WITH_PATCH}.jar" && \
    curl -L "${MAVEN_ROOT_URL}flink/flink-s3-fs-hadoop/${APACHE_FLINK_VERSION_WITH_PATCH}/flink-s3-fs-hadoop-${APACHE_FLINK_VERSION_WITH_PATCH}.jar" -o "${FLINK_LIB_DIR}flink-s3-fs-hadoop-${APACHE_FLINK_VERSION_WITH_PATCH}.jar" && \
    curl -L "${MAVEN_ROOT_URL}hadoop/hadoop-common/${HADOOP_VERSION}/hadoop-common-${HADOOP_VERSION}.jar" -o "${FLINK_LIB_DIR}hadoop-common-${HADOOP_VERSION}.jar" && \
    curl -L "${MAVEN_ROOT_URL}hadoop/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar" -o "${FLINK_LIB_DIR}hadoop-aws-${HADOOP_VERSION}.jar" && \
    curl -L "https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-10.0/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar" -o "${FLINK_LIB_DIR}flink-shaded-hadoop-2-uber-2.8.3-10.0.jar"; \
fi

# Set CLASSPATH environment variable
ENV CLASSPATH="${FLINK_LIB_DIR}*"

# Set JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-arm64

# Create core-site.xml
RUN echo '<?xml version="1.0" encoding="UTF-8"?>' > $FLINK_CONF_DIR/core-site.xml && \
    echo '<configuration>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <!-- S3A Filesystem Implementation -->' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <name>fs.s3a.impl</name>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    </property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <!-- AWS Credentials (using environment variables) -->' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <name>fs.s3a.access.key</name>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo "        <value>${AWS_ACCESS_KEY_ID}</value>" >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    </property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <name>fs.s3a.secret.key</name>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo "        <value>${AWS_SECRET_ACCESS_KEY}</value>" >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    </property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <!-- S3 Endpoint (optional) -->' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <name>fs.s3a.endpoint</name>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo "        <value>s3.${AWS_REGION}.amazonaws.com</value>" >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    </property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <!-- Path Style Access -->' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <name>fs.s3a.path.style.access</name>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <value>false</value>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    </property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <!-- Enable SSL (HTTPS) -->' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <name>fs.s3a.connection.ssl.enabled</name>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <value>true</value>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    </property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <name>fs.s3a.aws.credentials.provider</name>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</value>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    </property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    <property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <name>fs.s3a.connection.maximum</name>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '        <value>200</value>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '    </property>' >> $FLINK_CONF_DIR/core-site.xml && \
    echo '</configuration>' >> $FLINK_CONF_DIR/core-site.xml

# Create hdfs-site.xml
RUN echo '<?xml version="1.0" encoding="UTF-8"?>' > $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '<configuration>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '    <!-- Replication factor for HDFS files -->' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '    <property>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '        <name>dfs.replication</name>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '        <value>1</value>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '    </property>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '    <!-- Block size for HDFS files -->' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '    <property>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '        <name>dfs.blocksize</name>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '        <value>134217728</value>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '    </property>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '    <!-- Permissions for HDFS files -->' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '    <property>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '        <name>dfs.permissions</name>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '        <value>false</value>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '    </property>' >> $FLINK_CONF_DIR/hdfs-site.xml && \
    echo '</configuration>' >> $FLINK_CONF_DIR/hdfs-site.xml

# Create SQL Client Defaults
RUN echo 'catalogs:' > $FLINK_CONF_DIR/sql-client-defaults.yaml && \
    echo '  apache_kickstarter:' >> $FLINK_CONF_DIR/sql-client-defaults.yaml && \
    echo '    type: iceberg' >> $FLINK_CONF_DIR/sql-client-defaults.yaml && \
    echo '    property-version: 1' >> $FLINK_CONF_DIR/sql-client-defaults.yaml && \
    echo '    catalog-type: hadoop' >> $FLINK_CONF_DIR/sql-client-defaults.yaml && \
    echo "    warehouse: s3a://${AWS_S3_BUCKET}/warehouse" >> $FLINK_CONF_DIR/sql-client-defaults.yaml && \
    echo "    fs.s3a.endpoint: s3.${AWS_REGION}.amazonaws.com" >> $FLINK_CONF_DIR/sql-client-defaults.yaml && \
    echo "    fs.s3a.access.key: ${AWS_ACCESS_KEY_ID}" >> $FLINK_CONF_DIR/sql-client-defaults.yaml && \
    echo "    fs.s3a.secret.key: ${AWS_SECRET_ACCESS_KEY}" >> $FLINK_CONF_DIR/sql-client-defaults.yaml

# Set the entrypoint to Flink's entrypoint script
ENTRYPOINT ["/docker-entrypoint.sh"]